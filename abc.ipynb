{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTxv9yC2t-XY",
        "outputId": "89d04189-26f0-46b0-ce43-09d0c15e579d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/dataset.csv\")\n",
        "\n",
        "# Define the labels\n",
        "label_dict = {\n",
        "    \"Cy-Flaming\": 0,\n",
        "    \"Cy-Racism\": 1,\n",
        "    \"Cy-Threat\": 2,\n",
        "    \"Cy-Pull-a-Pig\": 3,\n",
        "    \"Not Bullying\": 4\n",
        "}\n",
        "\n",
        "# Convert labels to numeric\n",
        "df['label'] = df['label'].map(label_dict)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Convert to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        "))\n",
        "\n",
        "# Load the model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=5)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n",
        "\n",
        "# Evaluate the model\n",
        "results = model.evaluate(test_dataset.batch(16))\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(test_dataset.batch(16))\n",
        "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, predicted_labels, target_names=list(label_dict.keys())))"
      ],
      "metadata": {
        "id": "BulyWmxUH9DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db34bfb7-cfd4-4950-aefa-192fb2566612"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "138/138 [==============================] - 249s 2s/step - loss: 1.3425 - accuracy: 0.4314\n",
            "Epoch 2/3\n",
            "138/138 [==============================] - 226s 2s/step - loss: 0.8490 - accuracy: 0.6982\n",
            "Epoch 3/3\n",
            "138/138 [==============================] - 225s 2s/step - loss: 0.5409 - accuracy: 0.8182\n",
            "35/35 [==============================] - 18s 358ms/step - loss: 0.6543 - accuracy: 0.7586\n",
            "Test Loss: 0.6543238162994385\n",
            "Test Accuracy: 0.7586206793785095\n",
            "35/35 [==============================] - 16s 363ms/step\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Cy-Flaming       0.65      0.72      0.69       136\n",
            "    Cy-Racism       0.79      0.87      0.82       119\n",
            "    Cy-Threat       0.77      0.77      0.77        99\n",
            "Cy-Pull-a-Pig       0.84      0.66      0.74        89\n",
            " Not Bullying       0.81      0.76      0.78       108\n",
            "\n",
            "     accuracy                           0.76       551\n",
            "    macro avg       0.77      0.76      0.76       551\n",
            " weighted avg       0.76      0.76      0.76       551\n",
            "\n"
          ]
        }
      ]
    }
  ]
}